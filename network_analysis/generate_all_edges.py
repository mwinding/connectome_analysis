#%%
import os
import sys
try:
    os.chdir('/Volumes/GoogleDrive/My Drive/python_code/connectome_tools/')
    sys.path.append('/Volumes/GoogleDrive/My Drive/python_code/maggot_models/')
    sys.path.append('/Volumes/GoogleDrive/My Drive/python_code/connectome_tools/')
except:
    pass

from pymaid_creds import url, name, password, token
import pymaid

import numpy as np
import pandas as pd

import connectome_tools.process_matrix as pm
import connectome_tools.process_graph as pg
import networkx as nx

rm = pymaid.CatmaidInstance(url, token, name, password)
# %%
brain = pymaid.get_skids_by_annotation('mw brain neurons')
pairs = pd.read_csv('data/pairs/pairs-2021-04-06.csv', header = 0) # import pairs, manually determined with help from Heather Patsolic and Ben Pedigo's scripts

# duplicated right-side neurons to throw out for simplicity 
duplicated = pymaid.get_skids_by_annotation('mw duplicated neurons to delete')
duplicated_index = np.where(sum([pairs.rightid==x for x in duplicated])==1)[0]
pairs = pairs.drop(duplicated_index)

# import graphs of axon-dendrite split data; generated by Ben Pedigo's scripts
G = nx.readwrite.graphml.read_graphml('data/graphs/G.graphml', node_type=int)
Gad = nx.readwrite.graphml.read_graphml('data/graphs/Gad.graphml', node_type=int)
Gaa = nx.readwrite.graphml.read_graphml('data/graphs/Gaa.graphml', node_type=int)
Gdd = nx.readwrite.graphml.read_graphml('data/graphs/Gdd.graphml', node_type=int)
Gda = nx.readwrite.graphml.read_graphml('data/graphs/Gda.graphml', node_type=int)

'''
# identify duplicated neurons on right and their unduplicated left partner
duplicates = pymaid.get_skids_by_annotation('mw duplicated neurons')
left_duplicate_partner = list(np.unique(pairs.leftid[sum([pairs.rightid==x for x in duplicates])==1]))

# there exist a couple duplicated neurons on the right side
# this synthetically duplicates the singleton left side neuron so that they are matched
# synthetic neurons simply have a negative sign added to the original skid (i.e. 8700125=original, -8700125=synthetically duplicated)
for partner in left_duplicate_partner:
    for graph in [G, Gad, Gaa, Gdd, Gda]:
        if(partner in graph.nodes):
            in_edges = list(graph.in_edges(partner, data=True))
            out_edges = list(graph.out_edges(partner, data=True))
            in_edges_new = [(x[0], -partner, x[2]) for x in in_edges]
            out_edges_new = [(-partner, x[1], x[2]) for x in out_edges]
            all_edges = in_edges_new + out_edges_new

            if((partner, partner) in [(x[0], x[1]) for x in in_edges]):
                weight = in_edges[np.where([(x[0], x[1])==(partner, partner) for x in in_edges])[0][0]][2] # weight of self-edge if it exists
                all_edges = all_edges + [(-partner, -partner, weight)]

            for edge in all_edges:
                graph.add_edge(edge[0], edge[1], weight = edge[2]['weight'])

# change one of the duplicated pairs to match the new synthetic left side neuron
for partner in left_duplicate_partner:
    first_index = np.where(pairs.leftid == partner)[0][0]
    pairs.iloc[first_index, 0] = -pairs.iloc[first_index, 0]
'''

# generate adjacency matrices
adj_all = pd.DataFrame(nx.adjacency_matrix(G=G, weight = 'weight').todense(), columns = G.nodes, index = G.nodes)
adj_ad = pd.DataFrame(nx.adjacency_matrix(G=Gad, weight = 'weight').todense(), columns = Gad.nodes, index = Gad.nodes)
adj_aa = pd.DataFrame(nx.adjacency_matrix(G=Gaa, weight = 'weight').todense(), columns = Gaa.nodes, index = Gaa.nodes)
adj_dd = pd.DataFrame(nx.adjacency_matrix(G=Gdd, weight = 'weight').todense(), columns = Gdd.nodes, index = Gdd.nodes)
adj_da = pd.DataFrame(nx.adjacency_matrix(G=Gda, weight = 'weight').todense(), columns = Gda.nodes, index = Gda.nodes)

# export adjacency matrices
adj_all.to_csv('data/adj/all-neurons_all-all.csv')
adj_ad.to_csv('data/adj/all-neurons_axon-dendrite.csv')
adj_aa.to_csv('data/adj/all-neurons_axon-axon.csv')
adj_dd.to_csv('data/adj/all-neurons_dendrite-dendrite.csv')
adj_da.to_csv('data/adj/all-neurons_dendrite-axon.csv')

# import input data and export as simplified csv
meta_data = pd.read_csv('data/graphs/meta_data.csv', index_col = 0)
inputs = meta_data.loc[:, ['axon_input', 'dendrite_input']]

'''
# added input information for synthetic neurons
duplicate_inputs = inputs.loc[left_duplicate_partner, :].copy()
duplicate_inputs.index = [-x for x in duplicate_inputs.index]
inputs = pd.concat([inputs, duplicate_inputs])
'''

# exporting input data
inputs.to_csv('data/graphs/inputs.csv')

# %%
# prune out A1 neurons from adjacency matrices (optional)

# making some custom adjacencies without certain edge types
adj_allaa = adj_ad + adj_da + adj_dd
adj_ad_da = adj_ad + adj_da

#convert column names to int for easier indexing
adj_all.columns = adj_all.columns.astype(int) 
adj_ad.columns = adj_ad.columns.astype(int) 
adj_aa.columns = adj_aa.columns.astype(int) 
adj_da.columns = adj_da.columns.astype(int) 
adj_dd.columns = adj_dd.columns.astype(int) 
adj_allaa.columns = adj_allaa.columns.astype(int) 
adj_ad_da.columns = adj_ad_da.columns.astype(int) 

# remove A1 except for ascendings, also paritally differentiated neurons
A1_ascending = list(map(pymaid.get_skids_by_annotation, pymaid.get_annotated('mw brain ascendings').name))
A1_ascending = [x for sublist in A1_ascending for x in sublist]
A1 = pymaid.get_skids_by_annotation('mw A1 neurons paired')
A1_local = list(np.setdiff1d(A1, A1_ascending)) # all A1 without A1_ascending
prune_all = A1_local + pymaid.get_skids_by_annotation('mw partially differentiated') #+ pymaid.get_skids_by_annotation('mw brain accessory neurons')
pruned_index = [list(np.setdiff1d(x.index, A1_local))  for x in [adj_all, adj_ad, adj_aa, adj_dd, adj_da, adj_allaa, adj_ad_da]]

# remove all local A1 skids from adjacency matrix
adj_all = adj_all.loc[pruned_index[0], pruned_index[0]] 
adj_ad = adj_ad.loc[pruned_index[1], pruned_index[1]] 
adj_aa = adj_aa.loc[pruned_index[2], pruned_index[2]] 
adj_dd = adj_dd.loc[pruned_index[3], pruned_index[3]] 
adj_da = adj_da.loc[pruned_index[4], pruned_index[4]] 
adj_allaa = adj_allaa.loc[pruned_index[5], pruned_index[5]] 
adj_ad_da = adj_ad_da.loc[pruned_index[6], pruned_index[6]] 

# %%
# load adj matrices

adj_all_mat = pm.Adjacency_matrix(adj_all.values, adj_all.index, pairs, inputs, 'all')
adj_ad_mat = pm.Adjacency_matrix(adj_ad.values, adj_ad.index, pairs, inputs, 'ad')
adj_aa_mat = pm.Adjacency_matrix(adj_aa.values, adj_aa.index, pairs, inputs, 'aa')
adj_dd_mat = pm.Adjacency_matrix(adj_dd.values, adj_dd.index, pairs, inputs, 'dd')
adj_da_mat = pm.Adjacency_matrix(adj_da.values, adj_da.index, pairs, inputs, 'da')
adj_allaa_mat = pm.Adjacency_matrix(adj_allaa.values, adj_allaa.index, pairs, inputs, 'all-aa')
adj_ad_da_mat = pm.Adjacency_matrix(adj_ad_da.values, adj_ad_da.index, pairs, inputs, 'ad_da')

# %%
# generate all paired and nonpaired edges from each matrix with threshold
adjs = [adj_all_mat, adj_ad_mat, adj_aa_mat, adj_dd_mat, adj_da_mat, adj_allaa_mat, adj_ad_da_mat]
adjs_names = ['all', 'ad', 'aa', 'dd', 'da', 'all-aa', 'ad_da']

threshold = 0.01
left = pymaid.get_skids_by_annotation('mw left')
right = pymaid.get_skids_by_annotation('mw right')

for i, adj_mat in enumerate(adjs):
    matrix_pairs = pm.Promat.extract_pairs_from_list(adj_mat.skids, pairs)
    matrix_nonpaired = list(np.intersect1d(matrix_pairs[2].nonpaired, left+right)) # ignore unipolar neurons, not in set of brain neurons
    all_sources = list(matrix_pairs[0].leftid) + matrix_nonpaired

    all_edges_combined = adj_mat.threshold_edge_list(all_sources, matrix_nonpaired, threshold, left, right) # currently generates edge list for all paired -> paired/nonpaired, nonpaired -> paired/nonpaired
    all_edges_combined.to_csv(f'data/edges_threshold/{adjs_names[i]}_all_paired_edges.csv')

# %%
